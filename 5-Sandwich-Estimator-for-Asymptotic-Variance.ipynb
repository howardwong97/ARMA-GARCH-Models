{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandwich Estimator for Asymptotic Covariance\n",
    "\n",
    "*(Adapted from: http://www.stat.umn.edu/geyer/5601/notes/sand.pdf)*\n",
    "\n",
    "## Back to the Basics\n",
    "\n",
    "### Likelihood for One Observation\n",
    "\n",
    "Suppose we have a single data point, $x$, that is distributed according to a probability density function $f_\\theta$. The likelihood function $f_\\theta(x)$ is thought of as a function of the parameter $\\theta$ for fixed $x$, rather than the other way around:\n",
    "\n",
    "\\begin{equation}\n",
    "    L_x(\\theta) = f_{\\theta}(x)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    l_x(\\theta) = \\log f_{\\theta}(x)\n",
    "\\end{equation}\n",
    "\n",
    "### Likelihood for iid Observations\n",
    "\n",
    "Suppose we have a sequence of $iid$ random variables, $X_1, X_2, \\dots, X_n$, that have a common probability density function, $f_{\\theta}$:\n",
    "\n",
    "\\begin{equation}\n",
    "    f_{n,\\theta} (\\mathbf{x}) = \\prod_{i=1}^n f_{\\theta}(x_i)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    l_n(\\theta) = \\sum_{i=1}^{n} \\log f_{\\theta}(x_i)\n",
    "\\end{equation}\n",
    "\n",
    "The value of the parameter $\\theta$ that maximises the log-likelihood function is called the *maximum likelihood estimate*, $\\hat{\\theta}_n$, where the subscript $n$ denotes $iid$ data.\n",
    "\n",
    "### Log Likelihood Derivatives\n",
    "\n",
    "Consider the first two derivatives, $l'_x$ and $l''_x$. Differentiating the identity,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\int f_{\\theta} (x) \\,dx = 1,\n",
    "\\end{equation}\n",
    "\n",
    "under the integral to get the following results:\n",
    "\n",
    "\\begin{equation}\n",
    "    E_{\\theta} \\{ l'_x (\\theta) \\} = 0\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    var_{\\theta} \\{ l'_x (\\theta) \\} = -E_{\\theta} \\{ l''_x (\\theta) \\}\n",
    "\\end{equation}\n",
    "\n",
    "### Fisher Information\n",
    "\n",
    "The Fisher Information, $I(\\theta)$ is defined with either side of Equation (7):\n",
    "\n",
    "\\begin{equation}\n",
    "    I(\\theta) = var_{\\theta} \\{ l'_x (\\theta) \\}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    I(\\theta) = -E_{\\theta} \\{ l''_x (\\theta) \\}.\n",
    "\\end{equation}\n",
    "\n",
    "This is a way of measuring the amount of *information* that an observable random variable $X$ carries about an unknown parameter $\\theta$ of a distribution that models $X$. Formally, it is the *variance of the score*, or the *expected value of the observed information*.\n",
    "\n",
    "The Fisher information matrix is used to calculate the covariance matrices associated with maximum-likelihood estimates.\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "    I_n(\\theta) &= -E_{\\theta} \\{ l''_n (\\theta) \\} \\\\\n",
    "    &=  -E_{\\theta} \\left\\{ \\frac{d^2}{d\\theta^2} \\sum_{i=1}^n \\log f_{\\theta} (x_i) \\right\\} \\\\\n",
    "    &= -\\sum_{i=1}^n  E_{\\theta} \\left\\{ \\frac{d^2}{d\\theta^2} \\log f_{\\theta} (x_i) \\right\\} \\\\\n",
    "    &= -\\sum_{i=1}^n  E_{\\theta} \\left\\{ l''_1(\\theta) \\right\\} \\\\\n",
    "    &= n I_1 (\\theta) \\\\\n",
    "    \\therefore I_n (\\theta) &= n I_1(\\theta).\n",
    "\\end{align}\n",
    "\n",
    "### Asymptotics of Log Likelihood Derivatives\n",
    "\n",
    "#### Law or Large Numbers\n",
    "\n",
    "With $iid$ data, the *law of large numbers* applies to any average,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{1}{n} l'_n(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\frac{d}{d\\theta} \\log f_{\\theta}(x_i),\n",
    "\\end{equation}\n",
    "\n",
    "such that it will converge to its expectation, which as stated in Equation (6) should be equal to zero:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{1}{n} l'_n(\\theta) \\xrightarrow{P} 0\n",
    "\\end{equation}\n",
    "\n",
    "Similarly, applying the law of large numbers to the average:\n",
    "\n",
    "\\begin{equation}\n",
    "    -\\frac{1}{n} l''_n(\\theta) = -\\frac{1}{n} \\sum_{i=1}^n \\frac{d^2}{d\\theta^2} \\log f_{\\theta}(x_i),\n",
    "\\end{equation}\n",
    "\n",
    "says this will converge to its expectation, which by Equation (15) is just $I_1 (\\theta)$. Thus,\n",
    "\n",
    "\\begin{equation}\n",
    "    -\\frac{1}{n} l''_n(\\theta) \\xrightarrow{P} I_1 (\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "#### Central Limit Theorem\n",
    "\n",
    "If $X_1, X_2, \\dots, X_n$ are random samples each of size $n$ taken from a population with overall mean $\\mu$ and finite variance $\\sigma^2$ and if $\\bar{X}$ is the sample mean, the limiting form of the distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "    Z = \\left( \\frac{\\bar{X}_n - \\mu}{\\sigma \\mathbin{/} \\sqrt{n}}\\right) \\quad \\text{as } n \\rightarrow \\infty,\n",
    "\\end{equation}\n",
    "\n",
    "is the *standard normal distribution*. If we apply this to $l'_n(\\theta)$,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{1}{\\sqrt{n}} l'_n(\\theta) \\xrightarrow{D} N(0, I_1(\\theta))\n",
    "\\end{equation}\n",
    "\n",
    "where we notice that there is nothing to subtract because the expectation is equal to zero, and $\\sqrt{n} \\cdot \\left( \\frac{1}{n} \\right) = \\frac{1}{\\sqrt{n}}$\n",
    "\n",
    "### Asymptotics of MLE\n",
    "\n",
    "Assuming MLE is in the interior of the parameter space, the maximum log-likelihood occurs at:\n",
    "\n",
    "\\begin{equation}\n",
    "    l'_n (\\hat{\\theta}_n) = 0.\n",
    "\\end{equation}\n",
    "\n",
    "For large $n$, when $\\hat{\\theta}_n$ is close in value to $\\theta$ and, most importantly, assuming that it is a *consistent* estimator, $l'_n$ can be approximated by a Taylor series around $\\theta$:\n",
    "\n",
    "\\begin{equation}\n",
    "    l'_n (\\hat{\\theta}_n) \\approx l'_n(\\theta) + l''_n(\\theta)(\\hat{\\theta}_n - \\theta).\n",
    "\\end{equation}\n",
    "\n",
    "Since $l'_n (\\hat{\\theta}_n)$ is equal to zero, rearrange to find:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sqrt{n}(\\hat{\\theta}_n-\\theta) \\approx -\\frac{\\frac{1}{\\sqrt{n}}l'_n(\\theta)}{\\frac{1}{n}l''_n(\\theta)}.\n",
    "\\end{equation}\n",
    "\n",
    "According to the Central Limit Theorem, \n",
    "\n",
    "\\begin{equation}\n",
    "    -\\frac{\\frac{1}{\\sqrt{n}}l'_n(\\theta)}{\\frac{1}{n}l''_n(\\theta)} \\xrightarrow{D} \\frac{Z}{I_1(\\theta)}, \\quad \\text{where } Z \\sim N\\left(0, I_1(\\theta) \\right)\n",
    "\\end{equation}\n",
    "\n",
    "Now using the fact that $E(Z/c) = E(Z)/c$ and $var(Z/c) = var(Z)/c^2$, we get\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{Z}{I_1(\\theta)} \\sim N \\left( 0, I_1(\\theta)^{-1} \\right),\n",
    "\\end{equation}\n",
    "\n",
    "and crucially,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sqrt{n} \\left( \\hat{\\theta}_n - \\theta \\right) \\xrightarrow{D} N \\left( 0, I_1(\\theta)^{-1} \\right).\n",
    "\\end{equation}\n",
    "\n",
    "### Observed Fisher Information\n",
    "\n",
    "The *observed Fisher information* is written as\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{J}_n (\\theta) = -l''_n(\\theta).\n",
    "\\end{equation}\n",
    "\n",
    "Therefore, Equation (27) can be rewritten as:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sqrt{I_n(\\theta)} \\cdot \\left( \\hat{\\theta}_n - \\theta \\right) \\xrightarrow{D} N \\left( 0, 1 \\right).\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sqrt{\\hat{J}_n (\\theta)} \\cdot \\left( \\hat{\\theta}_n - \\theta \\right) \\xrightarrow{D} N \\left( 0, 1 \\right).\n",
    "\\end{equation}\n",
    "\n",
    "## Misspecified Maximum Likelihood Estimation\n",
    "\n",
    "### Modifying the Theory under Model Misspecification\n",
    "\n",
    "The true distribution has no parameter $\\theta$ because it is not in the model. We now write $E_g$ and $var_g$. In addition, under model misspecification, for $E_\\theta$ and $var_\\theta$ must be the same in order for the differentiation under the integral sign to work (Equations 5, 6, 7). \n",
    "\n",
    "Consider the expectation of the log likelihood\n",
    "\n",
    "\\begin{equation}\n",
    "    \\lambda_g (\\theta) = E_g \\{ l_X (\\theta) \\},\n",
    "\\end{equation}\n",
    "\n",
    "and suppose that the function $\\lambda_g$ achieves its maximum at some point $\\theta^*$. Assuming differentiation under the integral sign is possible,\n",
    "\n",
    "\\begin{equation}\n",
    "    E_g \\{ l'_X(\\theta^*) \\} = 0\n",
    "\\end{equation}\n",
    "\n",
    "Equation (7) is no longer valid under a misspecified model. The Fisher information is now defined with the two following equations:\n",
    "\n",
    "\\begin{equation}\n",
    "    V_n(\\theta) = var_g \\{l'_n(\\theta)\\} \\\\\n",
    "    J_n(\\theta) = -E_g \\{l''_n(\\theta)\\}\n",
    "\\end{equation}\n",
    "\n",
    "When the model is not misspecified, both $V_n$ and $J_n$ are simply equal to $I_n(\\theta)$. Similar to the result in Equation (15), we can now say that\n",
    "\n",
    "\\begin{equation}\n",
    "    V_n(\\theta) = n V_1(\\theta) \\\\\n",
    "    J_n(\\theta) = n J_1(\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "### Asymptotics under Model Misspecification\n",
    "\n",
    "Following a similar process as in Section 4.1, we conclude that\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sqrt{n} \\left( \\hat{\\theta}_n - \\theta^* \\right) \\xrightarrow{D} N \\left(0,J_1(\\theta^*)^{-1}V_1(\\theta^*)J_1(\\theta^*)^{-1})\\right),\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\theta}_n \\approx N \\left(\\theta^*,\\hat{J_n}(\\hat{\\theta}_n)^{-1}\\hat{V_n}(\\hat{\\theta}_n)\\hat{J_n}(\\hat{\\theta}_n)^{-1}\\right),\n",
    "\\end{equation}\n",
    "\n",
    "in which $V_n(\\theta)$ is replaced by an empirical estimate\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{V}_n(\\theta) = \\sum_{i=1}^{n} l'_n(\\theta)^2.\n",
    "\\end{equation}\n",
    "\n",
    "## The Sandwich Estimator\n",
    "\n",
    "The asymptotic variance here\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{J_n}(\\hat{\\theta}_n)^{-1}\\hat{V_n}(\\hat{\\theta}_n)\\hat{J_n}(\\hat{\\theta}_n)^{-1}\n",
    "\\end{equation}\n",
    "\n",
    "is also called the *sandwich estimator*. Under model misspecification, the asymptotic variance is no longer simply the \"inverse Fisher information\". We calculate the asymptotic variance to allow us to compute t-stats for our estimates.\n",
    "\n",
    "### Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.append('tools')\n",
    "\n",
    "import armagarch as ag\n",
    "from numdifftools import Hessian\n",
    "from yahooquery import Ticker\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use historical S&P500 data as an example\n",
    "prices = Ticker('^GSPC').history(period='10y', interval='1d')\n",
    "prices.reset_index(inplace=True)\n",
    "prices.drop('symbol', axis=1, inplace=True)\n",
    "prices.set_index('date', inplace=True)\n",
    "prices.index = pd.to_datetime(prices.index)\n",
    "prices.to_csv('data/SP500.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date\n",
       "2010-10-11    0.000146\n",
       "2010-10-12    0.003811\n",
       "2010-10-13    0.007096\n",
       "2010-10-14   -0.003648\n",
       "2010-10-15    0.002025\n",
       "Name: adjclose, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute log returns from adjusted close\n",
    "log_returns = np.log(prices['adjclose'] / prices['adjclose'].shift(1))[1:]\n",
    "log_returns[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use armagarch to fit model\n",
    "X = log_returns.values[-500:] * 100.0  # scale values by 100 for optimizer convergence\n",
    "fit = ag.fit_model(X, 't', 3, 3, gjr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute asymptotic variance using the 'sandwich estimator':\n",
    "\n",
    "# Compute V\n",
    "jac = Jacobian(ag.t_negative_llh)(fit.x, 3, 3, X, True, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 12)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = jac.T.dot(jac)\n",
    "V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = Hessian(ag.t_negative_llh)(fit.x, 3, 3, X, True, None)\n",
    "Jinv = np.linalg.inv(J)\n",
    "asymptotic_variance = np.diag(Jinv.dot(V).dot(Jinv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.59436711e-07, 2.50209907e-04, 2.23213749e-05, 1.55967314e-04,\n",
       "       1.90447259e-06, 1.03294612e-09, 4.45232801e-06, 4.46330774e-04,\n",
       "       9.57730142e-05, 3.76288344e-03, 2.56107691e-04, 7.86332743e-02])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asymptotic_variance = np.diag(Jinv.dot(V).dot(Jinv))\n",
    "asymptotic_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter   Estimate       Std. Err.      T-stat\n",
      "c           0.000359        0.000600     0.59938\n",
      "phi0        -0.699662        0.015818    -44.23197\n",
      "phi1        0.812933        0.004725     172.06572\n",
      "phi2        0.825918        0.012489     66.13332\n",
      "theta0      0.615972        0.001380     446.34818\n",
      "theta1      -0.842383        0.000032    -26210.22954\n",
      "theta2      -0.793667        0.002110    -376.13610\n",
      "omega       0.083374        0.021127     3.94639\n",
      "alpha       0.055586        0.009786     5.67991\n",
      "gamma       0.266674        0.061342     4.34730\n",
      "beta        0.811077        0.016003     50.68168\n",
      "v           3.000000        0.280416     10.69838\n"
     ]
    }
   ],
   "source": [
    "output = np.vstack(\n",
    "    (fit.x, np.sqrt(asymptotic_variance), fit.x / np.sqrt(asymptotic_variance))\n",
    ").T\n",
    "print('Parameter   Estimate       Std. Err.      T-stat')\n",
    "param = ['c','phi0', 'phi1', 'phi2', 'theta0', 'theta1', 'theta2',\n",
    "         'omega', 'alpha','gamma', 'beta', 'v']\n",
    "for i in range(len(param)):\n",
    "    print('{0:<11} {1:>0.6f}        {2:0.6f}    {3: 0.5f}'.format(param[i],\n",
    "           output[i,0], output[i,1], output[i,2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": "5",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

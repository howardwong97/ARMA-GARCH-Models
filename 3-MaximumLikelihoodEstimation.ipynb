{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation of ARMA-GARCH\n",
    "\n",
    "One possible approach of fitting an ARMA-GARCH model is to perform a maximum likelihood estimation (MLE) for the conditional mean (ARMA), then an MLE of the conditional variance (GARCH). However, joint estimation is preferred. In the first stage of ARMA estimation, there is an implicit assumption of conditional homoskedasticity. It is contradicted in the second stage when you explicitly model conditional heteroskedasticity using GARCH.\n",
    "\n",
    "## An Example: GARCH(1,1) with Normal Distribution\n",
    "\n",
    "Recall a GARCH(1,1) model is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sigma_t^2 = \\alpha_0 + \\alpha_1 \\epsilon_{t-1}^2 + \\beta_1 \\sigma_{t-1}^2,\n",
    "\\end{equation}\n",
    "\n",
    "and the *log-likelihood* function for a normally distributed random variable is:\n",
    "\n",
    "\\begin{equation}\n",
    "    L = - \\frac{1}{2} \\sum_{t=1}^T \\left( \\ln \\sigma_t^2 + \\left(\\frac{\\epsilon_{t}}{\\sigma_t} \\right)^2 \\right),\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def garch(alpha0, alpha1, beta1, epsilon):\n",
    "    T = len(epsilon)\n",
    "    sigma_2 = np.zeros(T)\n",
    "    \n",
    "    for t in range(T):\n",
    "        if t == 0:\n",
    "            sigma_2[t] = alpha0 / (1 - alpha1 - beta1) # initialize as unconditional variance\n",
    "        else:\n",
    "            sigma_2[t] = alpha0 + alpha1*epsilon[t-1]**2 + beta1*sigma_2[t-1]\n",
    "            \n",
    "    return sigma_2\n",
    "    \n",
    "def garch_neg_loglike(params, epsilon):\n",
    "    T = len(epsilon)\n",
    "    alpha0 = params[0]\n",
    "    alpha1 = params[1]\n",
    "    beta1 = params[2]\n",
    "    sigma_2 = garch(alpha0, alpha1, beta1, epsilon)\n",
    "    NegLogL = -0.5 * np.sum(-np.log(sigma_2) - epsilon**2/sigma_2)  # negative sign for minimization\n",
    "    return NegLogL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 1475.3801857119552\n",
       " hess_inv: <3x3 LbfgsInvHessProduct with dtype=float64>\n",
       "      jac: array([-0.00350155, -0.00422915, -0.00513865])\n",
       "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
       "     nfev: 76\n",
       "      nit: 16\n",
       "     njev: 19\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([0.06084818, 0.13416952, 0.82254072])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "data = pd.read_csv('data/top10_logreturns.csv', index_col=0, parse_dates=True)['D05.SI'] * 100  # scaled for ease of optimization\n",
    "bounds = tuple((0.0001, None) for i in range(3))\n",
    "params_initial = (0.1, 0.05, 0.92)\n",
    "cons = (\n",
    "    {'type': 'ineq', 'func': lambda x: np.array(x)},\n",
    "    {'type': 'ineq', 'func': lambda x: 1-x[1]-x[2]+0.00000000000001}\n",
    ")  \n",
    "\n",
    "res = minimize(garch_neg_loglike, params_initial, args=(data), bounds=bounds, options={'disp': True})\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Example: ARMA(1,1) with Normal Distrbitution\n",
    "\n",
    "We utilise the same log-likelihood function defined above, but this time we use an ARMA(1,1) model,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\epsilon_t = r_t - \\phi_0 - \\phi_1 r_{t-1} - \\theta_1 \\epsilon_{t-1},\n",
    "\\end{equation}\n",
    "\n",
    "which has been rearranged to solve for $\\epsilon_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arma(phi0, phi1, theta1, r):\n",
    "    T = len(r)\n",
    "    epsilon = np.zeros(T)\n",
    "    \n",
    "    for t in range(T):\n",
    "        if t == 0:\n",
    "            epsilon[t] = r[t] - np.mean(r)\n",
    "        else:\n",
    "            epsilon[t] = r[t] - phi0 - phi1*r[t-1] - theta1*epsilon[t-1]\n",
    "    \n",
    "    return epsilon\n",
    "\n",
    "def arma_neg_loglike(params, r):\n",
    "    T = len(r)\n",
    "    phi0 = params[0]\n",
    "    phi1 = params[1]\n",
    "    theta1 = params[2]\n",
    "    epsilon = arma(phi0, phi1, theta1, r)\n",
    "    NegLogL = -0.5 * np.sum(-np.log(r.var()) - epsilon**2/r.var())\n",
    "    return NegLogL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bounds = tuple((0.0001, None) for i in range(3))\n",
    "params_initial = (0.1, 0.05, 0.92)\n",
    "cons = ({'type': 'ineq', 'func': lambda x: np.array(x)})  \n",
    "\n",
    "res = minimize(arma_neg_loglike, params_initial, args=(data), bounds=bounds, options={'disp': True})\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting ARMA-GARCH with MLE\n",
    "\n",
    "Based on some preliminary research, many sources suggest estimating the *ARMA* process first, followed by modelling the innovations with GARCH. However, this will most likely lead to inconsistent parameter estimates. In fitting an ARMA model, there is an assumption made about the *conditional variance* - it is constant. This is clearly not the case when the process is assumed to follow that of GARCH. This is especially an issue when it comes to order determination for the ARMA model - the ACF and PACF confidence bounds will be invalid given the GARCH-type residuals. Therefore, parameter determination via MLE must be performed for both ARMA and GARCH *simultaneously*. This simply involves substituting the *conditional mean* component from ARMA and the *conditional variance* component from GARCH into the log-likelihood equation and minimizing with `scipy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def armagarch_negloglike(params, r):\n",
    "    T = len(r)\n",
    "    phi0 = params[0]\n",
    "    phi1 = params[1]\n",
    "    theta1 = params[2]\n",
    "    alpha0 = params[3]\n",
    "    alpha1 = params[4]\n",
    "    beta1 = params[5]\n",
    "    \n",
    "    epsilon = arma(phi0, phi1, theta1, r)\n",
    "    sigma_2 = garch(alpha0, alpha1, beta1, epsilon)\n",
    "    \n",
    "    NegLogL = -0.5 * np.sum(-np.log(sigma_2) - epsilon**2/sigma_2)\n",
    "    \n",
    "    return NegLogL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-1a29fe4b8cf4>:13: RuntimeWarning: invalid value encountered in log\n",
      "  NegLogL = -0.5 * np.sum(-np.log(sigma_2) - epsilon**2/sigma_2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "      fun: 1468.1974203477923\n",
       " hess_inv: <6x6 LbfgsInvHessProduct with dtype=float64>\n",
       "      jac: array([ 0.00613909,  0.39729002, -0.0017053 ,  0.01248281, -0.00616183,\n",
       "        0.02321485])\n",
       "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
       "     nfev: 427\n",
       "      nit: 42\n",
       "     njev: 61\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([4.94361056e-02, 1.00000000e-04, 5.64613715e-02, 5.45187055e-02,\n",
       "       1.27127949e-01, 8.33942576e-01])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounds = tuple((0.0001, None) for i in range(6))\n",
    "params_initial = tuple(0.0001 for _ in range(6))\n",
    "cons = ({'type': 'ineq', 'func': lambda x: np.array(x)},\n",
    "        {'type': 'ineq', 'func': lambda x: 1-x[-1]-x[-2]+0.00000000000001})  \n",
    "\n",
    "res = minimize(armagarch_negloglike, params_initial, args=(data), bounds=bounds, options={'disp': True})\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2948.3948406955847"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AIC calculate as 2k - 2LogL or \n",
    "# AIC = 2k + 2NegLogL\n",
    "\n",
    "AIC = 2 * len(params_initial) + 2* res.fun\n",
    "AIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "` model = Model(mean='ARMA', var='GARCH', data=data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": "3",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
